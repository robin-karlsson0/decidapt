<?xml version="1.0" encoding="UTF-8"?>
<launch>
    <arg name="action_server_name" default="action_decision_action_server" description="Name of the action decision action server"/>
    <arg name="action_decision_topic" default="/action_decision" description="Topic for action decision messages"/>
    <arg name="log_pred_io_pth" default="" description="Directory path where LLM prediction IO (input, output) will be logged as individual JSON files" />
    <arg name="inference_server_type" default="tgi" description="Type of inference server: 'tgi' or 'vllm'"/>
    <arg name="inference_server_url" default="http://localhost:8000" description="Inference server URL for LLM inference"/>
    <arg name="max_tokens" default="1" description="Maximum tokens for Action Decision LLM output (single token)"/>
    <arg name="llm_temp" default="0.0" description="Temperature for LLM next-token output distribution"/>
    <arg name="llm_seed" default="14" description="Seed for reproducible LLM results"/>
    <arg name="max_retries" default="3" description="Maximum number of retries for LLM inference in case of failure"/>

    <node pkg="action_cycle_controller" exec="action_decision" name="action_decision">
        <param name="action_server_name" value="$(var action_server_name)"/>
        <param name="action_decision_topic" value="$(var action_decision_topic)"/>
        <param name="log_pred_io_pth" value="$(var log_pred_io_pth)"/>
        <param name="inference_server_type" value="$(var inference_server_type)"/>
        <param name="inference_server_url" value="$(var inference_server_url)"/>
        <param name="max_tokens" value="$(var max_tokens)"/>
        <param name="llm_temp" value="$(var llm_temp)"/>
        <param name="llm_seed" value="$(var llm_seed)"/>
        <param name="max_retries" value="$(var max_retries)"/>
    </node>
</launch>